\documentclass{l4proj}
\usepackage{fullpage}
\usepackage{txfonts}
\usepackage{graphicx}
\begin{document}
\graphicspath{ {/} }

\title{%
  The QUIC Transport Protocol}
\author{Emily Band, 2038561b}
\date{28 March 2018}
\maketitle

\educationalconsent

\tableofcontents

\pagebreak

\chapter{Introduction}

\pagenumbering{arabic}

Networked applications are effectively restricted to a choice of two transport protocols: TCP or UDP. These protocols are decades old, and were designed to support a much smaller volume of traffic than the vast amounts generated by modern Internet. Any inefficiency in these protocols generates a large amount of redundant traffic, a significant problem when systems have fewer clock cycles available to process each incoming packet.

Ossification of transport protocols has been enforced as a result of interference from middleboxes, which are ill-equipped to deal with new transport protocols built directly on top of IP; SCTP and DCCP are examples of failed attempts to deploy more effective replacements for TCP and UDP respectively.

This paper explores the development of the QUIC protocol, an attempt to subvert this ossification by building a userspace transport protocol on top of UDP. The use of a familiar protocol enables widespread deployment, while allowing sufficient freedom to build loss-recovery features, parallel streams for asynchronous content delivery, and a reduced latency handshake process compared to regular TCP stacks.

This project details the process of creating Mercury, a QUIC library implemented in Rust. Although Rust is not currently used for any implementations of QUIC\cite{quic-implementations}, its region-based approach to memory management gives Mercury several advantages over existing solutions: a safer, more maintainable alternative to the implementations which use C or C++, and one which has faster performance than the ones which use Go.\cite{govsrust}

Another key focus of this project is reflecting on attempting to implement QUIC as a solo developer. The IETF specification for this protocol is extensive, with eight active documents on the IETF Datatracker page for the QUIC working group.\cite{quic-documents} Learning and constructing a QUIC library is a very different experience for a student with considerable time constraints compared to working as part of a team of experienced engineers at an influential company.

This paper contains the following chapters:
\begin{itemize}
\item{\textbf{Rust: Safer Systems Programming:} reflects on the process of learning a language with an unconventional approach to memory management, and evaluates its advantages and disadvantages.}
\item{\textbf{The IETF: Open-Sourcing the Internet:} details the structure, conventions, and politics behind creating Internet standards.}
\item{\textbf{The QUIC Protocol:} explores previous attemps to improve TCP, the mechanics of QUIC, and its benefits.}
\item{\textbf{Project Scope and Requirements:} explains which sections of the QUIC protocol could be feasibly implemented in the time given.}
\item{\textbf{Creating Mercury: A Rust-Based QUIC Library:} explores the technical details of creating a QUIC library for the chosen areas of the specification, along with reflections on which sections could be improved.}
\item{\textbf{Status and Future Work:} summarises the current capabilities of the library, the areas of the specification which will be implemented in a future iteration of the Mercury project, and the issues which are likely to affect this.}
\item{\textbf{Conclusions:} discusses the use of Rust as a tool for low-level networking, interacting with the IETF as a solo participant, the potential of moving transport protocols from the kernel into userspace, and the impact of encrypted transport protocols on the Internet as a whole.}
\end{itemize}


\pagebreak


\chapter{Rust: Safer Systems Programming}
C and C++ are still the languages of choice for systems programming - the vast majority of current QUIC implementations are written in one of these.\cite{quic-implementations} Although programs written in these languages are fast and efficient, they are prone to memory leaks and segmentation faults due to incorrect memory management. Race conditions are common in multithreaded C and C++ programs due to restrictions on concurrent memory access not being strictly enforced.

Rust uses region-based memory management and ownership to allow efficient, predictable use of resources while minimising the risk of memory leaks, illegal memory accesses, and race conditions. This focus on safety and performance is perfect for a multithreaded transport protocol, but no IETF implementation has taken advantage of it yet.

\section{Memory Management in Rust}
There are several strategies which programming languages use to for memory management.

[Discuss Python's abstraction of malloc and free - low-effort for programmers and good for small numbers of objects with lots of references, but terrible performance for lots of small objects. Requires intervention to detect and eliminate isolated cycles.]

[Discuss Java's 'stop the world' approach and generational collection - new used as abstraction for malloc. No effort required from the programmer but performance suffers. Makes Java unsuitable for systems programming due to slowdown and unpredictable use of resources.]

[C's use of malloc and free - fast and efficient when done correctly, but requires caution from the programmer. Compiler does not check for memory leaks, segmentation faults, or buffer overflows caused by incorrect memory management. Multithreaded applications are even more complex to manage.]
[Include sample code showing a buffer overflow error]

Rust uses region-based memory management to free allocated heap memory as soon as the compiler detects that a value can no longer be accessed. In practice, this means that a Rust program will implicitly call \texttt{drop()} on a variable which is going out of scope when a function ends, and will free the heap memory which was allocated for its associated value.

[Include a code fragment and diagram to illustrate this]

References can be used to allow a function to borrow values, allowing it to use them without removing data from the heap after exiting:

[Include a code fragment and diagram to illustrate mutable and immutable borrowing]


\section{A Steep Learning Curve}
Rust has acquired a reputation for being difficult to learn.\cite{rust-difficulty2}\cite{rust-difficulty3}\cite{rust-difficulty1} I would broadly agree with these sentiments, with the following four areas being the most notable obstacles: ownership, lifetimes, strings, and compiler errors.

\subsection{Ownership}
Attempting to use a variable after it no longer points to a value on the heap will result in a compiler error:

[Include example of attempting to use value after move]

While this is easier to debug than encountering a segmentation fault or undefined behaviour during runtime, it is initially confusing why this throws an error in the first place when this code would be legal in many other popular languages. The answer to this lies in Rust's approach to ownership of data, which is best explained through comparing two similar traits: \texttt{Copy} and \texttt{Clone}.

\subsubsection{\texttt{Copy} and \texttt{Clone}}
Values which have a constant value known at compile time, such as integers, are assigned memory on the stack instead of the heap and do not need to call \texttt{drop()}. The \texttt{Copy} trait is present for variables which have stack-allocated values (ie. anything which doesn't implement the \texttt{Drop} trait). Making a copy of a stack-allocated value is guaranteed to be a computationally cheap operation given its known size at compile time, so stack-allocated values can be easily assigned to multiple variables without ownership conflicts:

[Include code fragment showing copy behaviour]

[Include diagram of pointer and data changes for copy]

\texttt{Clone}, by contrast, is an explicit function call which creates a deep copy of a value on the heap and allows a variable to take ownership of it:

[Include code fragment showing .clone()] 

[Include diagram of pointers and data manipulations for clone]

Attempting to treat a heap value in the same way as a stack value creates a scenario where \texttt{drop()} would be called twice on the same section of memory after the function terminates: Rust's version of a double free error in C.\cite{double-free} This illegal conflict of ownership throws the compiler error shown in (fig no. here):

[Include diagram of illegal pointer configuration]

Assigning a new pointer to a copy of a stack value is guaranteed to be a computationally cheap operation compared to copying a potentially large section of heap-allocated data, so custom types should use \texttt{Copy} rather than \texttt{Clone} where possible.

\subsubsection{\texttt{Box<T>}}

\subsection{Lifetimes}

\subsection{\texttt{String} and \texttt{str}}

\subsection{Compiler Errors}
[Rust has a list of 644 compiler errors, all documented on the official site.\cite{compiler-error-list} 629 of these can still be emitted by the compiler.

There's even functionality in Cargo to detail a specific compiler error using the \texttt{--explain} command.]

\section{\texttt{if} and \texttt{match}}

\section{\texttt{Result} and \texttt{Option}}
[These abstractions are an additional layer of safety. Simplifies error handling - a wide range of errors can be encapsulated with \texttt{Err}, values which may or may not be present can be handled using \texttt{Some}. \texttt{.unwrap()} can cause runtime errors, but learning how to avoid these is relatively straightforward. \texttt{.expect(\&str)} can be helpful for debugging.]

\section{Traits}
[Allows for a lot of flexibility for custom types: define functionality for one or two base functions, and Rust will know how a range of variants on these should work. Discuss Read and Write as examples w/ code fragments.]

\section{\texttt{unsafe} Blocks}
[Discussion of raw pointers, doubly-linked lists, and FFI]

\section{Cargo}
[Makes building projects and including code from other crates very easy. Rustdoc generates documentation which follows a consistent format - makes unfamiliar libraries easier to learn how to use and reduces the potential for errors in the documentation (typos in code, outdated types, etc).]

\section{Summary}
['Is the trade-off from Rust worth it?' - Very much so. Initial learning curve is harsh, but the stability, speed, efficient use of resources, and ease of debugging is worth it long-term. Dismissing languages for having unconventional ideas impedes progress.]


\pagebreak


\chapter{The IETF: Open-Sourcing the Internet}

The Internet Engineering Task Force is a community open to anyone who wishes to participate in developing Internet standards. Subjects of interest are first informally discussed as a 'birds of a feather' (BoF) meeting. If the participants agree that the topic is complex enough to warrant further work, a BoF can apply to become a working group.

Each working group is assigned to an area, with the group's work being overseen by an area director. Drafts being submitted for consideration as standards (RFCs) are reviewed by the Internet Engineering Steering Group (IESG) before being officially released. The Internet Assigned Numbers Authority ensures that Internet protocols developed by working groups in the IETF do not have conflicting values.

\section{QUIC Working Group}


[Talk about mailing lists, github repos, and meetings - anyone is free to participate, no restrictions on entry]

\section{Rough Consensus and Running Code}
\begin{quotation}
"We reject: kings, presidents and voting. We believe in: rough consensus and running code."
\end{quotation}

The IETF focuses on creating working implementations of projects undertaken by working groups. These projects are complex and it would be far too easy for them to become stalled through indecision if prototypes were not actively being developed and updated as required.

In the QUIC working group, development is guided by gaining rough consensus about issues on the mailing list, the GitHub issue tracker, and at meetings. No counted votes are taken, which eliminates any delays caused by the administrative overhead required for voting and allows for more subtlety in adopted solutions than a checkbox can offer. Favouring a group opinion instead of obeying an absolute authority significantly also reduces the chances of flawed drafts being submitted for IESG review.

The flexibility of rough consensus has advantages for large projects like QUIC. The working group's milestones have been repeatedly pushed back due to unexpected complexities discovered through mailing list discussion and interoperability testing. Failing to meet deadlines is a matter of concern for the IESG, but delays are sometimes necessary to resolve a problem before it becomes too ingrained and becomes an obstacle to later development. Having the ability to change deadlines and project scope ensures the working group's official output will be secure, maintainable, and compatible with the wider Internet infrastructure.

%'Does the process work?
[Lots of energetic discussion on the mailing list and at meetings - no conditions on participation allows people to be honest in their views. Definitely an air of some people showing off, but ego isn't unique to the IETF. Group dynamics, corporate involvement, and public accountability reduces potential for genuinely toxic behaviour.

The current system works with a relatively small, easily moderated number of participants, but could easily turn into a Stack Overflow-like culture if thousands of people suddenly joined.]

\section{Corporate and Political Interests}
[GQUIC variant exists distinct from IETF-standardised QUIC (which in turn could be argued to be heavily influenced by Mozilla) - transport which has the potential to vary service-by-service is new territory, could have similar implications to net neutrality?

Some developments do end up being hijacked for political purposes (eg. Differentiated Services is key component in attacking net neutrality, RFCs 2474 and 2475). This could be intentional for participants representing corporations, or lack of awareness from engineers who are solely focused on technical challenges. Dedicated IRTF group set up to assess the human rights issues associated with IETF work (Human Rights Protocol Considerations Research Group).

Need to find out what kind of checks IESG does for drafts before they become RFCs - any checking for socio-political stuff with HRPC or is it purely technical issues?]

\section{Summary}
The IETF has a self-enforced hierarchy to co-ordinate the release of official standards, but it allows a lot of freedom as a participant: everyone is free to post to mailing lists, submit pull requests on a working group repo, or even form a BoF.

However, the main drivers of progress in the QUIC working group are representatives from large corporations. While the IETF is a good venue for these companies to co-operate on developing a mutually compatible protocol, it is difficult in practice for individual participants to gain enough experience to make a significant contribution: without funding which allows someone to study the QUIC group's extensive documents as part of their working day, the average person will not be able to gain enough knowledge about the protocol before the next revision occurs. This is not an intentional barrier to entry imposed by the companies involved, simply a consequence of the complex nature of the project.

[Will check if this is a trend for most working groups at IETF 101. I suspect it is.]

\pagebreak


\chapter{The QUIC Protocol}

The QUIC (Quick UDP Internet Connections) protocol was originally developed by Google as a more efficient alternative to TCP. While previous work on improving transport protocols has focused on kernel modification, QUIC is a userspace protocol which uses UDP as a base and has stream-like behaviour and loss recovery integrated at application level. TLS 1.3 is used to encrypt QUIC packet payloads for data security, and sections of the header to evade middlebox interference.

[Include diagram of stack]

Google has been gradually deploying its own version of the protocol in the wild since 2013, with egress traffic over QUIC from its servers more than doubling after enabling it for the Android YouTube app in late 2016.\cite{gquic-development} As of 2018, 42.1\% of Google traffic and up to 9.1\% of total Internet traffic is transported using QUIC.\cite{quic-traffic-2018}

In June 2016, the IETF established the QUIC working group with the aim of creating a standardised version of the protocol. Representatives from Mozilla, Google, Facebook, and Apple are active on the working group's mailing list, and a range of IETF-compliant implementations are also being developed by teams at these companies.\cite{quic-implementations}

\section{Related Work}
[Standard TCP stacks are slow and most do not use TLS 1.3 as standard

NetMap and StackMap have been attempted as kernel-level improvements on TCP, but they have restrictions (taking up an entire networking interface, potential problems with encryption)]

\subsection{Protection For Legacy Systems}
[Recent operating systems have TCP stacks with theoretically comparable performance to QUIC (TCP fast-open), but some systems cannot be upgraded (eg. NHS systems forced to continue using Windows XP). Userspace protocols allow at least some degree of improved performance and protection.]

\subsection{Overcoming Ossification}
[Reasons why middlebox-enforced ossification exists and strategies to avoid it (encryption, greasing)]

"Encryption \textit{is} our defence against network ossification [...] That's the only defence we have left against network middleboxes at this point." \cite{iyengar-ossification}

\section{Anatomy of a QUIC Connection}

\subsection{UDP Sockets}

\subsection{QUIC Packets}
QUIC uses two types of packet: LongHeader and ShortHeader.

\subsection{LongHeader}
Used for exchanging 0-RTT key-protected data and initial connection setup.

[Include packet format diagram and explain sections]

\subsection{ShortHeader}
Used for exchanging 1-RTT key-protected data.

[Include packet format diagram and explain sections]

\subsection{Handshake Process}

\subsection{Payload Frames}

\subsection{Connection Teardown}

\section{Summary}


\pagebreak


\chapter{Project Scope and Requirements}
[Can only implement Handshake in time given - most implementations are created by experienced IETF participants, not novice undergrads]


\pagebreak


\chapter{Creating Mercury: A Rust-Based QUIC Library}
[NOTE: Mercury is quic-08 draft compliant]

\section{The \texttt{mio} Crate}
Enough flexibility for the project, simple to learn. \texttt{tokio} is more complex but may be a better choice for multithreaded work, might convert to this to prepare for work with multiple streams. 

\section{\texttt{header} module}
[Detail the custom functions here, encoding and decoding process]

\section{Porting TLS 1.3 to UDP}
%FFI, complexity of NSS and OpenSSL
\subsection{Selecting a TLS Library}
Mozilla NSS and OpenSSL are highly customisable, but far too heavyweight for the project. picotls has been developed specifically for QUIC, but is written in C. Could use FFI to integrate it into this project, but adapting rustls preserves memory safety through minimising use of raw pointers, and is a better learning experience for understanding how TLS is supposed to work in QUIC.

\section{Modifying the \texttt{rustls} Server Example}

\subsection{Events Poll}

\subsection{Simulating Connections in UDP}

\subsection{Custom Socket Type: \texttt{QuicSocket}}

\subsection{Consolidating Handshake TLS Messages}

\section{Modifying the \texttt{rustls} Client Example}


\pagebreak


\chapter{Status and Future Work}

\section{Current Status}

\section{Version Negotiation}

\section{Frames}

\section{Streams and Concurrency}

\section{Loss Recovery}

\section{Interop Testing}
\subsection{Unexpected IPv6 Behaviour}

\section{Version 10}
[Lack of backwards compatibility between drafts has been a setback - changes between v7 and v8 were significant, changing to v10 will require a redesign]

\section{Deciding between DTLS and TLS}

\subsection{Conflicting Advice from TLS Working Group}


\pagebreak


\chapter{Conclusions}
\section{Rust for Low-Level Networking}
[Excellent choice after getting past the initial learning curve, huge potential if enough people support it.]

\section{Working on a Solo Project in the IETF}
[Impossible to complete a project alone as a student, but very good as a learning experience.

Overall, valuable insight into protocol development - being freely allowed to read mailing lists and watch meeting sessions provided valuable insights for the project. Keen to get more involved.]

\section{Potential of Userspace Protocols}
[Choices beyond basic TCP and UDP - applications can talk to a shim layer (eg. NEAT) to select a transport according to their current needs, no longer have to set up sockets directly. QUIC is just one of these possibilities.]

[Corporate influences over transport as mentioned in IETF section - could lead to segregated Internet]

\section{The Future of QUIC}
[Currently very fashionable and gaining influence. Direct backing from Google and heavy influence from Mozilla and Facebook suggests it will become widely adopted.]

%Concerns about encryption
\subsection{A Less Open Internet?}
[Difficult to monitor due to end-to-end encryption - great for user privacy, but much less data about network traffic available to track network performance. Restricted monitoring available with spin bit.

Encryption by default is going to be interesting with current UK government stance against E2E encryption.]

\bibliographystyle{plain}
\bibliography{references.bib}

\end{document}






